{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9bfb526",
   "metadata": {},
   "source": [
    "<font size=\"+3\" color='#053c96'><h2><center> Classically Punk</h2></center></font>\n",
    "<figure>\n",
    "<center><img src =\"https://images.unsplash.com/photo-1487180144351-b8472da7d491?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=872&q=80\" width = \"750\" height = '600' alt=\"Classical Punk\"/>\n",
    "<font size=\"0\" color='#053c96'><h4><center> Photo Credit: Unsplash</h4></center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8aca5",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color='#053c96'><b> Contributor</b></font>  \n",
    "<font size=\"+0\" ><b> Umar Kabir</b></font>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c866556",
   "metadata": {},
   "source": [
    "<a id='table-of-contents'></a>\n",
    "[Table of Contents](#table-of-contents)\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "  * [Overview](#overview)\n",
    "  * [Problem Statement](#problem-statement)\n",
    "  * [Objectives](#goals)\n",
    "- [Importing Libraries](#importing-dependencies)\n",
    "- [Data](#data)\n",
    "- [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "  * [Data Exploration](#data-exploration)\n",
    "  * [Data Visualization](#data-visualization)\n",
    "  * [Summary Statistics](#summary-statistics)\n",
    "  * [Feature Correlation](#feature-correlation)\n",
    "- [Data Preparation](#data-preparation)\n",
    "  * [Data Cleaning](#data-cleaning)\n",
    "  * [Feature Engineering](#feature-engineering)\n",
    "  * [Data Transformation](#data-transformation)\n",
    "- [Modeling](#modeling)\n",
    "  * [Model Selection](#model-selection)\n",
    "  * [Model Training](#model-training)\n",
    "  * [Model Evaluation](#model-evaluation)\n",
    "  * [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "- [Results](#results)\n",
    "  * [Analysis Results](#analysis-results)\n",
    "  * [Model Performance](#model-performance)\n",
    "  * [Feature Importance](#feature-importance)\n",
    "  * [Implications](#implications)\n",
    "- [Conclusion](#conclusion)\n",
    "  * [Summary](#summary)\n",
    "  * [Limitations](#limitations)\n",
    "  * [Recommendations](#recommendations)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836292fd",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "<font size=\"+2\" color='#053c96'><b> 1. Introduction</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "Classically Punk is a music genre classification project that combines different elements of music to develop a machine learning model that predicts music genre.  \n",
    "\n",
    "In this project, we aim to use machine learning techniques to automatically classify different musical genres, from audio snippets. To achieve this, we will need to find a library that can read music files and extract features from them, such as tempo, pitch, and melody. We will then use these features to train a machine learning model that can classify different genres of music.  \n",
    "\n",
    "The deliverables for this project include a presentation with slides on how we classified the music, as well as assumptions, implications, and other important information, and code that the DevOps team can push to production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06537a61",
   "metadata": {},
   "source": [
    "<a id='overview'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 1.1 Overview</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "The project aims to develop a machine learning application that can automatically classify different musical genres from audio snippets. The main steps involved in the project include finding a library that can read music files and extract features from them, identifying relevant features for classification, and training a machine learning model to classify different genres of music. The project also involves handling large data sets and analyzing media files to generate data and identify patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a08599",
   "metadata": {},
   "source": [
    "<a id='problem-statement'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 1.2 Problem Statement</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "The problem statement for this project is the difficulty in manually classifying large collections of music into different genres. This process can be time-consuming and prone to errors, as it requires a deep understanding of the characteristics of each genre. Furthermore, as the amount of music available online continues to grow, it becomes increasingly challenging to keep up with the task of categorizing music by hand. The goal of this project is to develop a machine learning application that can automate the process of music classification, making it faster, more accurate, and scalable. The application will use features extracted from audio files to train a machine learning model that can classify different genres of music, including Classically Punk, without the need for human intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65055b3b",
   "metadata": {},
   "source": [
    "<a id='goals'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 1.3 Objectives</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "1. To identify a suitable library for reading music files and extracting features from them.\n",
    "2. To determine relevant features that can be used for music genre classification, such as tempo, pitch, and melody.\n",
    "3. To preprocess the audio data, such as removing noise and converting it into a format suitable for machine learning algorithms.\n",
    "4. To train a machine learning model on the audio data using a suitable algorithm, such as neural networks or decision trees.\n",
    "5. To evaluate the performance of the machine learning model using appropriate metrics, such as accuracy, precision, and recall.\n",
    "6. To optimize the model's performance by tuning hyperparameters and experimenting with different algorithms.\n",
    "7. To develop a user-friendly interface for the application that allows users to upload audio files and receive genre classification results.\n",
    "8. To present the results of the project, including the classification accuracy and the features that were most relevant for genre classification.\n",
    "9. To deliver code that can be easily deployed by the DevOps team for use in a production environment.\n",
    "10. - Speech activity detection: Speech activity detection is the task of identifying the segments of an audio signal that contain speech. This can be done by looking for changes in the energy of the signal, the zero crossing rate, or the pitch.\n",
    "- Speaker identification: Speaker identification is the task of identifying the speaker of an audio signal. This can be done by looking for features that are unique to each speaker, such as the vocal tract shape or the way that they pronounce certain words.\n",
    "- Music genre classification: Music genre classification is the task of classifying an audio signal into a particular genre of music. This can be done by looking for features that are commonly associated with different genres of music, such as the tempo, the pitch, or the rhythm.\n",
    "- Sound event detection: Sound event detection is the task of identifying the different types of sounds that are present in an audio signal. This can be done by looking for changes in the energy of the signal, the spectral content of the signal, or the temporal structure of the signal.\n",
    "- Automatic music transcription: Automatic music transcription is the task of converting an audio recording of music into a musical score. This can be done by looking for features that are associated with different musical notes, such as the pitch, the duration, and the timbre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c48e75",
   "metadata": {},
   "source": [
    "<a id='importing-dependencies'></a>\n",
    "<font size=\"+2\" color='#053c96'><b> 2. Importing Libraries</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ac95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Insert the parent path relative to this notebook so we can import from the src folder.\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from src.dependencies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a29aa",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "<font size=\"+2\" color='#053c96'><b> 3. Data</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "The dataset was used for the well-known paper in genre classification \"Musical genre classification of audio signals\" by G. Tzanetakis and P. Cook in IEEE Transactions on Audio and Speech Processing 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734db0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "data = []\n",
    "\n",
    "for genre in genres:\n",
    "    genre_dir = f'../data/genres/{genre}'\n",
    "    for filename in os.listdir(genre_dir):\n",
    "        if filename.endswith('.wav'):\n",
    "            audio_path = os.path.join(genre_dir, filename)\n",
    "            y, sr = librosa.load(audio_path)\n",
    "\n",
    "            # Extracting the Spectral features for each file\n",
    "            # MFCCs\n",
    "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "            # Spectral centroid\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "\n",
    "            # Spectral bandwidth\n",
    "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "\n",
    "            # Spectral rolloff\n",
    "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "\n",
    "            # Spectral contrast\n",
    "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "\n",
    "            # Spectral flatness\n",
    "            spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
    "\n",
    "            # Harmonicity\n",
    "            harmonicity = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "\n",
    "            # Percussiveness\n",
    "            percussiveness = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "\n",
    "            # Spectral flux\n",
    "            spectral_flux = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "\n",
    "            # Spectral slope\n",
    "            spectral_slope = librosa.feature.chroma_vqt(y=y, sr=sr, intervals='equal')\n",
    "\n",
    "            \n",
    "            # Extracting the Temporal features for each file\n",
    "            # Zero crossing rate\n",
    "            zcr = librosa.feature.zero_crossing_rate(y=y)\n",
    "\n",
    "            # Energy\n",
    "            energy = librosa.feature.delta(data=y)\n",
    "\n",
    "            # Tempo\n",
    "            tempo = librosa.feature.tempo(y=y, sr=sr)\n",
    "\n",
    "            # Pitch\n",
    "            pitch = librosa.feature.poly_features(y=y, sr=sr)\n",
    "\n",
    "            # Loudness\n",
    "            loudness = librosa.feature.tonnetz(y=y)\n",
    "\n",
    "            # Duration\n",
    "            duration = librosa.get_duration(y=y)\n",
    "\n",
    "            # Complexity\n",
    "            complexity = librosa.feature.tempogram_ratio(y=y)\n",
    "\n",
    "\n",
    "            # Timbre\n",
    "            timbre = librosa.feature.tempogram(y=y)\n",
    "            \n",
    "            # Extracting the Energy features for each file\n",
    "\n",
    "            \n",
    "\n",
    "            # Spectral energy\n",
    "            stft = librosa.stft(y=y)\n",
    "\n",
    "            spectral_energy = np.abs(stft) ** 2\n",
    "\n",
    "            # Temporal energy\n",
    "            # Set parameters for windowing\n",
    "            window_size = 1024  # Adjust as needed\n",
    "            hop_size = 512     # Adjust as needed\n",
    "\n",
    "            # Calculate the number of windows\n",
    "            num_windows = (len(y) - window_size) // hop_size + 1\n",
    "\n",
    "            # Initialize an array to store temporal energy values\n",
    "            temporal_energy = np.zeros(num_windows)\n",
    "\n",
    "            # Calculate temporal energy for each window\n",
    "            for i in range(num_windows):\n",
    "                window_start = i * hop_size\n",
    "                window_end = window_start + window_size\n",
    "                audio_window = y[window_start:window_end]\n",
    "                \n",
    "                # Calculate squared amplitude as temporal energy\n",
    "                temporal_energy[i] = np.sum(audio_window ** 2)\n",
    "\n",
    "\n",
    "            # Energy flux\n",
    "            energy_flux = []\n",
    "            for i in range(len(y) - window_size):\n",
    "                window = y[i : i + window_size]\n",
    "                energy_diff = np.diff(window)  # Calculate the difference between consecutive samples\n",
    "                energy_flux.append(0.5 * np.sum(energy_diff ** 2))\n",
    "\n",
    "            # Energy distribution\n",
    "            energy_distribution = np.abs(stft) ** 2\n",
    "\n",
    "            # Sum energy across time frames to get frequency-based energy distribution\n",
    "            frequency_energy_distribution = np.sum(energy_distribution, axis=1)\n",
    "\n",
    "            # Normalize energy distribution to get a probability distribution\n",
    "            normalized_energy_distribution = frequency_energy_distribution / np.sum(frequency_energy_distribution)\n",
    "\n",
    "            # Calculate energy entropy\n",
    "            energy_entropy = -np.sum(normalized_energy_distribution * np.log2(normalized_energy_distribution + 1e-12))  # Avoid log(0)\n",
    "\n",
    "\n",
    "\n",
    "            # Extracting the Chroma features for each file\n",
    "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "            features = [genre, filename, mfccs, spectral_centroid, spectral_bandwidth, spectral_rolloff, spectral_contrast, spectral_flatness,\n",
    "            harmonicity, percussiveness, spectral_flux, spectral_slope, zcr, energy, tempo, pitch, loudness, duration, complexity, timbre,\n",
    "            spectral_energy, temporal_energy, energy_flux, energy_distribution, energy_entropy]\n",
    "\n",
    "            # Adding all the features to a dataframe\n",
    "            #features = [genre, filename, beats, sr, central_moments, zero_crossing_rate[0], rmse[0], tempo, spectral_contrast, spectral_rolloff, mfccs, chroma, spectral_centroid, spectral_bandwidth]\n",
    "            data.append(features)\n",
    "df = pd.DataFrame(data, columns=[\n",
    "    \"genre\",\n",
    "    \"filename\",\n",
    "    \"mfccs\",\n",
    "    \"spectral_centroid\",\n",
    "    \"spectral_bandwidth\",\n",
    "    \"spectral_rolloff\",\n",
    "    \"spectral_contrast\",\n",
    "    \"spectral_flatness\",\n",
    "    \"harmonicity\",\n",
    "    \"percussiveness\",\n",
    "    \"spectral_flux\",\n",
    "    \"spectral_slope\",\n",
    "    \"zcr\",\n",
    "    \"energy\",\n",
    "    \"tempo\",\n",
    "    \"pitch\",\n",
    "    \"loudness\",\n",
    "    \"duration\",\n",
    "    \"complexity\",\n",
    "    \"timbre\",\n",
    "    \"spectral_energy\",\n",
    "    \"temporal_energy\",\n",
    "    \"energy_flux\",\n",
    "    \"energy_distribution\",\n",
    "    \"energy_entropy\",\n",
    "])\n",
    "\n",
    "#df = pd.DataFrame(data, columns=['Genre', 'Filename', 'Beats', 'SR', 'Central Moments', 'Zero Crossing Rate', 'RMSE', 'Tempo', 'Spectral Contrast', 'Spectral Roll-off', 'MFCC', 'Chroma', 'Spectral Centroid', 'Spectral Bandwidth'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654781d6",
   "metadata": {},
   "source": [
    "#### Spectral features extracted from the audio files:\n",
    "\n",
    "- Mel-frequency cepstral coefficients (MFCCs): MFCCs are a popular set of spectral features that are often used in speech recognition and music classification tasks. MFCCs are derived from the Mel-scale spectrogram, which is a representation of the frequency content of the audio signal that is more perceptually relevant than the linear frequency spectrogram.\n",
    "- Spectral centroid: The spectral centroid is the frequency at which the spectral energy is concentrated. The spectral centroid can be used to identify different instruments in a piece of music, or to track the pitch of a voice.\n",
    "- Spectral bandwidth: The spectral bandwidth is the width of the frequency band that contains most of the spectral energy. The spectral bandwidth can be used to identify different types of sounds, such as vowels and consonants.\n",
    "- Spectral rolloff: The spectral rolloff is the frequency below which a certain percentage of the spectral energy is concentrated. The spectral rolloff can be used to identify different types of sounds, such as male and female voices.\n",
    "- Spectral contrast: Spectral contrast is a measure of the difference between the spectral energy at different frequencies. Spectral contrast can be used to identify different types of sounds, such as musical instruments and speech.\n",
    "- Spectral flatness: Spectral flatness is a measure of the uniformity of the spectral energy. Spectral flatness can be used to identify different types of sounds, such as noise and speech.\n",
    "- Harmonicity: Harmonicity is a measure of the ratio of the harmonic energy to the total energy in the audio signal. Harmonicity can be used to identify different types of sounds, such as musical instruments and speech.\n",
    "- Percussiveness: Percussiveness is a measure of the energy in the audio signal that is concentrated at low frequencies. Percussiveness can be used to identify different types of sounds, such as drums and footsteps.\n",
    "- Spectral flux: Spectral flux is a measure of the change in the spectral energy over time. Spectral flux can be used to identify different events in an audio recording, such as the start of a word or the end of a musical phrase.\n",
    "- Spectral slope: Spectral slope is a measure of the rate of change of the spectral energy over time. Spectral slope can be used to identify different types of sounds, such as vowels and consonants.\n",
    "- Spectral kurtosis: Spectral kurtosis is a measure of the peakedness of the spectral energy distribution. Spectral kurtosis can be used to identify different types of sounds, such as musical instruments and speech.\n",
    "\n",
    "#### Temporal features extracted from the audio files:\n",
    "\n",
    "- Zero crossing rate: The zero crossing rate is the number of times the audio signal crosses the zero axis per second. The zero crossing rate can be used to identify different types of sounds, such as percussive sounds and vowels.\n",
    "- Energy: The energy of the audio signal is a measure of its overall loudness. The energy can be used to identify different types of sounds, such as speech and music.\n",
    "- Tempo: The tempo is the speed of the audio signal. The tempo can be used to identify different types of music, such as classical music and dance music.\n",
    "- Pitch: The pitch of the audio signal is the frequency at which the sound is oscillating. The pitch can be used to identify different types of sounds, such as male and female voices.\n",
    "- Loudness: The loudness of the audio signal is a measure of its perceived intensity. The loudness can be used to identify different types of sounds, such as loud noises and quiet sounds.\n",
    "- Duration: The duration of the audio signal is the length of time it takes for the sound to play. The duration can be used to identify different types of sounds, such as short sounds and long sounds.\n",
    "- Complexity: The complexity of the audio signal is a measure of the number of different frequencies that are present in the sound. The complexity can be used to identify different types of sounds, such as simple sounds and complex sounds.\n",
    "- Rhythm: The rhythm of the audio signal is the pattern of the sound's loudness and pitch over time. The rhythm can be used to identify different types of music, such as rock music and jazz music.\n",
    "- Timbre: The timbre of the audio signal is the quality of the sound that distinguishes it from other sounds of the same pitch and loudness. The timbre can be used to identify different types of musical instruments, such as a violin and a piano.\n",
    "\n",
    "#### Energy features extracted from the audio files:\n",
    "\n",
    "- Root mean square energy (rmse): The root mean square energy is the square root of the mean of the squared signal values. The rmse is a measure of the overall loudness of the signal.\n",
    "- Peak signal-to-noise ratio (PSNR): The peak signal-to-noise ratio is the ratio between the peak signal value and the noise level. The PSNR is a measure of the quality of the signal.\n",
    "- Spectral energy: The spectral energy is the energy of the signal at each frequency. The spectral energy can be used to identify different types of sounds, such as speech and music.\n",
    "- Temporal energy: The temporal energy is the energy of the signal over time. The temporal energy can be used to identify different types of sounds, such as percussive sounds and vowels.\n",
    "- Energy flux: The energy flux is the rate of change of the energy of the signal over time. The energy flux can be used to identify different types of sounds, such as speech and music.\n",
    "- Energy distribution: The energy distribution is the distribution of the energy of the signal over time. The energy distribution can be used to identify different types of sounds, such as speech and music.\n",
    "- Energy entropy: The energy entropy is a measure of the randomness of the energy of the signal over time. The energy entropy can be used to identify different types of sounds, such as speech and music.\n",
    "- Energy autocorrelation: The energy autocorrelation is a measure of the similarity of the energy of the signal at different time points. The energy autocorrelation can be used to identify different types of sounds, such as speech and music.\n",
    "- Energy kurtosis: The energy kurtosis is a measure of the peakedness of the energy distribution. The energy kurtosis can be used to identify different types of sounds, such as speech and music.\n",
    "- Energy skewness: The energy skewness is a measure of the asymmetry of the energy distribution. The energy skewness can be used to identify different types of sounds, such as speech and music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('classical_punk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('classical_punk.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47144d6c",
   "metadata": {},
   "source": [
    "<a id='exploratory-data-analysis'></a>\n",
    "<font size=\"+2\" color='#053c96'><b> 4. Exploratory Data Anaysis</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b442d4",
   "metadata": {},
   "source": [
    "<a id='data-exploration'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 4.1 Data Exploration</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f477f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e12f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa452bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1933e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Genre'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a1e0c",
   "metadata": {},
   "source": [
    "This function loads and plays an audio file of a specific genre and number using the librosa library. It takes two arguments, genre and num, which specify the genre of the audio and the number of the audio file within that genre, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('blues', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ef5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('classical', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f5394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('country', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af33b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('disco', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e740bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('hiphop', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('jazz', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75992ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('metal', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75714a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('pop', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('reggae', '00024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cf24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio('rock', '00024')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f4dd5",
   "metadata": {},
   "source": [
    "<a id='data-visualization'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 4.2 Data Visualization</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb346b1",
   "metadata": {},
   "source": [
    "This code generates a frequency bar chart of the 'Tempo' column in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):\n",
    "    df = df.to_frame(index=False)\n",
    "df = df.reset_index().drop('index', axis=1, errors='ignore')\n",
    "df.columns = [str(c) for c in df.columns]\n",
    "tempo_counts = df['Tempo'].value_counts().reset_index()\n",
    "tempo_counts.columns = ['Tempo', 'Frequency']\n",
    "tempo_counts = tempo_counts.sort_values(['Frequency', 'Tempo'], ascending=[False, True])\n",
    "tempo_counts = tempo_counts[:100]\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.barplot(x='Frequency', y='Tempo', data=tempo_counts)\n",
    "ax.set(xlabel='Frequency', ylabel='Tempo', title='Tempo Value Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ff0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_waveform(x, num):\n",
    "    # Load WAV file\n",
    "    wav_file = f'/Users/umarkabir/Documents/Qwasar/Classical Punk/genres/{x}/{x}.{num}.wav'\n",
    "    y, sr = librosa.load(wav_file)\n",
    "# Create x-axis values\n",
    "    time = librosa.times_like(y, sr=sr)\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.lineplot(x=time, y=y)\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title(f'Sample waveform for {x}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411d650",
   "metadata": {},
   "source": [
    "This function loads a WAV file of a specific genre and number, and generates a sample waveform plot using the librosa and plotly libraries. It takes two arguments, x and num, which specify the genre of the audio and the number of the audio file within that genre, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('blues', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957903dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('classical', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee228c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('country', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('disco', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876baecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('hiphop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a189dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('jazz', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('metal', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('pop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('reggae', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_waveform('rock', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9306be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectogram(x, num):\n",
    "    # Load audio file\n",
    "    audio_path = f'/Users/umarkabir/Documents/Qwasar/Classical Punk/genres/{x}/{x}.{num}.wav'\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Calculate spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "\n",
    "    # Convert to decibels\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    ax = sns.heatmap(S_dB, cmap='viridis')\n",
    "\n",
    "    # Set x and y axis labels\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "    # Set figure title\n",
    "    ax.set_title(f'Sample spectrogram for {x}')\n",
    "\n",
    "    # Show figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d5198",
   "metadata": {},
   "source": [
    "This function loads a WAV file of a specific genre and number, and generates a sample spectrogram plot using the librosa and plotly libraries. It takes two arguments, x and num, which specify the genre of the audio and the number of the audio file within that genre, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('blues', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6419f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('classical', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('country', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('disco', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('hiphop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb37527",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('jazz', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('metal', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14edbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('pop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('reggae', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b091da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectogram('rock', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def show_sr(x, num):\n",
    "    # Load audio file\n",
    "    audio_path = f'/Users/umarkabir/Documents/Qwasar/Classical Punk/genres/{x}/{x}.{num}.wav'\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Compute spectral rolloff\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "\n",
    "    # Create plot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(spectral_rolloff, color='blue')\n",
    "    ax.set(title=f'Sample spectral rolloff for {x}', xlabel='Frame', ylabel='Frequency (Hz)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6796c",
   "metadata": {},
   "source": [
    "This function show_sr(x, num) loads an audio file and computes the spectral rolloff. It then creates a Plotly line plot of the spectral rolloff values with the x-axis representing the frame and the y-axis representing frequency in Hz. The title of the plot is set to \"Sample spectral rolloff for x\", where x is the name of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57524963",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('blues', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('classical', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('country', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d922f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('disco', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('hiphop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93428",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('jazz', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48545f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('metal', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('pop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438cd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('reggae', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9255df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sr('rock', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f44e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_chroma(x, num):\n",
    "    # Load audio file\n",
    "    audio_path = f'/Users/umarkabir/Documents/Qwasar/Classical Punk/genres/{x}/{x}.{num}.wav'\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Compute chroma feature\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "\n",
    "    # Create time axis in seconds\n",
    "    time = librosa.frames_to_time(np.arange(chroma.shape[1]), sr=sr)\n",
    "\n",
    "    # Create chroma note names\n",
    "    chroma_note_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(chroma, index=chroma_note_names, columns=time)\n",
    "\n",
    "    # Plot heatmap using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df, cmap='viridis', xticklabels=50, yticklabels=1)\n",
    "    plt.title(f'Sample chroma feature for {x}')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Chroma Note')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4e47c",
   "metadata": {},
   "source": [
    "This function takes in two arguments x and num representing the music genre and the song number, respectively. It then loads the corresponding audio file and computes the chroma feature using librosa's chroma_stft function. It creates a time axis in seconds using frames_to_time function, and chroma note names as a list. It then creates a heatmap trace using go.Heatmap with time as the x-axis, chroma_note_names as the y-axis, and chroma as the z-axis. Finally, it sets the layout with appropriate x and y axis titles, and a title for the figure. It shows the resulting figure using fig.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('blues', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112da3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('classical', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0239bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('country', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ecfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('disco', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('hiphop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('jazz', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067380b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('metal', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52443761",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('pop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb4749",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('reggae', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e07a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chroma('rock', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_zcr(x, num):\n",
    "    # Load audio file\n",
    "    audio_path = f'/Users/umarkabir/Documents/Qwasar/Classical Punk/genres/{x}/{x}.{num}.wav'\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Compute zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "\n",
    "    # Plot with Seaborn\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    sns.lineplot(data=zcr[0], ax=ax)\n",
    "    ax.set(title=f'Zero Crossing Rate for {x} Genre', xlabel='Time (s)', ylabel='ZCR')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85dd14",
   "metadata": {},
   "source": [
    "The show_zcr function takes in two arguments: x, which represents the genre of the music file, and num, which represents the number of the music file. It calculates the zero-crossing rate of the audio file and plots it using Plotly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('blues', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc43cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('classical', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ae409",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('country', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('disco', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9917b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('hiphop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b38f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('jazz', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('pop', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('reggae', '00090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e37627",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_zcr('rock', '00090')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642835c",
   "metadata": {},
   "source": [
    "<a id='summary-statistics'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 4.3 Summary Statistics</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd55d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_kurt(data, col):\n",
    "    # Calculate skewness and kurtosis of Income column\n",
    "    _skewness = skew(data[col])\n",
    "    _kurtosis = kurtosis(data[col])\n",
    "\n",
    "    # Create histogram of Income column with mean, median, and mode\n",
    "    sns.histplot(data=data, x=col, kde=True)\n",
    "    plt.axvline(data[col].mean(), color='r', linestyle='--', label='Mean')\n",
    "    plt.axvline(data[col].median(), color='g', linestyle='--', label='Median')\n",
    "    plt.axvline(data[col].mode()[0], color='b', linestyle='--', label='Mode')\n",
    "    plt.legend()\n",
    "\n",
    "    # Add text annotation for skewness and kurtosis values\n",
    "    plt.annotate('Skewness: {:.2f}'.format(_skewness), xy=(0.5, 0.9), xycoords='axes fraction')\n",
    "    plt.annotate('Kurtosis: {:.2f}'.format(_kurtosis), xy=(0.5, 0.85), xycoords='axes fraction')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f48ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'blues'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'classical'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'country'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3758ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'disco'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d668351",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'hiphop'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'jazz'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'metal'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cc118",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'pop'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'reggae'], 'Tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_kurt(df[df['Genre'] == 'rock'], 'Tempo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea1afa",
   "metadata": {},
   "source": [
    "<a id='feature-correlation'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 4.4 Feature Correlation</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b02106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "\n",
    "# Set figure size and font sizes\n",
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "sns.set(font_scale=1.9)\n",
    "\n",
    "# Plot heatmap with adjusted color map\n",
    "sns.heatmap(df_corr, cmap='coolwarm', annot=True, center=0, square=True)\n",
    "\n",
    "# Adjust font size of features\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=35)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=35)\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title('Correlation Matrix', fontsize=30)\n",
    "plt.xlabel('Features', fontsize=20)\n",
    "plt.ylabel('Features', fontsize=20)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1727a626",
   "metadata": {},
   "source": [
    "<a id='data-preparation'></a>\n",
    "<font size=\"+2\" color='#053c96'><b> 5. Data Preparation</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fc006",
   "metadata": {},
   "source": [
    "<a id='data-cleaning'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 5.1 Data Cleaning</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab2b48",
   "metadata": {},
   "source": [
    "<a id='feature-engineering'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 5.2 Feature Engineering</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ce3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Beats Mean'] = df['Beats'].apply(lambda x: np.mean(x))\n",
    "df['Central Moments Mean'] = df['Central Moments'].apply(lambda x: np.mean(x))\n",
    "df['Zero Crossing Rate Mean'] = df['Zero Crossing Rate'].apply(lambda x: np.mean(x))\n",
    "df['RMSE Mean'] = df['RMSE'].apply(lambda x: np.mean(x))\n",
    "df['Spectral Contrast Mean'] = df['Spectral Contrast'].apply(lambda x: np.mean(x))\n",
    "df['Spectral Roll-off Mean'] = df['Spectral Roll-off'].apply(lambda x: np.mean(x))\n",
    "df['MFCC Mean'] = df['MFCC'].apply(lambda x: np.mean(x))\n",
    "df['Chroma Mean'] = df['Chroma'].apply(lambda x: np.mean(x))\n",
    "df['Spectral Centroid Mean'] = df['Spectral Centroid'].apply(lambda x: np.mean(x))\n",
    "df['Spectral Bandwidth Mean'] = df['Spectral Bandwidth'].apply(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Beats Var'] = df['Beats'].apply(lambda x: np.var(x))\n",
    "df['Central Moments Var'] = df['Central Moments'].apply(lambda x: np.var(x))\n",
    "df['Zero Crossing Rate Var'] = df['Zero Crossing Rate'].apply(lambda x: np.var(x))\n",
    "df['RMSE Var'] = df['RMSE'].apply(lambda x: np.var(x))\n",
    "df['Spectral Contrast Var'] = df['Spectral Contrast'].apply(lambda x: np.var(x))\n",
    "df['Spectral Roll-off Var'] = df['Spectral Roll-off'].apply(lambda x: np.var(x))\n",
    "df['MFCC Var'] = df['MFCC'].apply(lambda x: np.var(x))\n",
    "df['Chroma Var'] = df['Chroma'].apply(lambda x: np.var(x))\n",
    "df['Spectral Centroid Var'] = df['Spectral Centroid'].apply(lambda x: np.var(x))\n",
    "df['Spectral Bandwidth Var'] = df['Spectral Bandwidth'].apply(lambda x: np.var(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aa6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Beats Std'] = df['Beats'].apply(lambda x: np.std(x))\n",
    "df['Central Moments Std'] = df['Central Moments'].apply(lambda x: np.std(x))\n",
    "df['Zero Crossing Rate Std'] = df['Zero Crossing Rate'].apply(lambda x: np.std(x))\n",
    "df['RMSE Std'] = df['RMSE'].apply(lambda x: np.std(x))\n",
    "df['Spectral Contrast Std'] = df['Spectral Contrast'].apply(lambda x: np.std(x))\n",
    "df['Spectral Roll-off Std'] = df['Spectral Roll-off'].apply(lambda x: np.std(x))\n",
    "df['MFCC Std'] = df['MFCC'].apply(lambda x: np.std(x))\n",
    "df['Chroma Std'] = df['Chroma'].apply(lambda x: np.std(x))\n",
    "df['Spectral Centroid Std'] = df['Spectral Centroid'].apply(lambda x: np.std(x))\n",
    "df['Spectral Bandwidth Std'] = df['Spectral Bandwidth'].apply(lambda x: np.std(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cff5001",
   "metadata": {},
   "source": [
    "<a id='data-transformation'></a>\n",
    "<font size=\"+1\" color='#780404'><b> 5.3 Data Transformation</b></font>  \n",
    "[back to top](#table-of-contents)\n",
    "<a id='modeling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09402f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_col = df['Genre']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(genre_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target dataset\n",
    "#y = df['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8952a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(['Genre', 'Filename', 'Unnamed: 0', 'Beats', 'SR',\n",
    "             'Central Moments', 'Zero Crossing Rate', 'RMSE',\n",
    "             'Spectral Contrast', 'Spectral Roll-off', 'MFCC',\n",
    "             'Chroma', 'Spectral Centroid', 'Spectral Bandwidth'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a TF-IDF object instance\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pareto Principle Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size= 0.3,\n",
    "                                                    random_state= 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43353f",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "\n",
    "<font size=\"+2\" color='#053c96'><b> 6. Modeling</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ddcd8",
   "metadata": {},
   "source": [
    "<a id='model-selection'></a>\n",
    "\n",
    "<font size=\"+1\" color='#780404'><b> 6.1 Model Selection</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac15df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Support Vector Machines (SVM)\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', OneVsRestClassifier(SVC(kernel='rbf', C=1, gamma='scale')))\n",
    "])\n",
    "\n",
    "# K-Means Clustering\n",
    "kmeans_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('kmeans', KMeans(n_clusters=10))\n",
    "])\n",
    "\n",
    "# K-Nearest Neighbors (KNN)\n",
    "knn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "# Convolutional Neural Networks (CNNs)\n",
    "cnn_pipeline = Pipeline([\n",
    "    ('clf', Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_pipeline.named_steps['clf'].compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadadaf",
   "metadata": {},
   "source": [
    "<a id='model-training'></a>\n",
    "\n",
    "<font size=\"+1\" color='#780404'><b> 6.2 Model Training</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025cade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM pipeline\n",
    "svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Fit K-Means pipeline\n",
    "kmeans_pipeline.fit(X_train)\n",
    "\n",
    "# Fit KNN pipeline\n",
    "knn_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d33277",
   "metadata": {},
   "source": [
    "<a id='model-evaluation'></a>\n",
    "\n",
    "<font size=\"+1\" color='#780404'><b> 6.3 Model Evaluation</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists for performance metrics and confusion matrices\n",
    "f1_list = []\n",
    "pr_list = []\n",
    "rc_list = []\n",
    "acc_list = []\n",
    "cm_list = []\n",
    "\n",
    "# Evaluate additional models\n",
    "# evaluate the SVM pipeline\n",
    "\n",
    "y_pred_svm = svm_pipeline.predict(X_test)\n",
    "f1_list.append(f1_score(y_test, y_pred_svm, average='weighted'))\n",
    "pr_list.append(precision_score(y_test, y_pred_svm, average='weighted'))\n",
    "rc_list.append(recall_score(y_test, y_pred_svm, average='weighted'))\n",
    "acc_list.append(accuracy_score(y_test, y_pred_svm))\n",
    "cm_list.append(confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "# evaluate the K-Means pipeline\n",
    "y_pred_kmeans = kmeans_pipeline.predict(X_test)\n",
    "f1_list.append(f1_score(y_test, y_pred_kmeans, average='weighted'))\n",
    "pr_list.append(precision_score(y_test, y_pred_kmeans, average='weighted'))\n",
    "rc_list.append(recall_score(y_test, y_pred_kmeans, average='weighted'))\n",
    "acc_list.append(accuracy_score(y_test, y_pred_kmeans))\n",
    "cm_list.append(confusion_matrix(y_test, y_pred_kmeans))\n",
    "\n",
    "# evaluate the KNN pipeline\n",
    "y_pred_knn = knn_pipeline.predict(X_test)\n",
    "f1_list.append(f1_score(y_test, y_pred_knn, average='weighted'))\n",
    "pr_list.append(precision_score(y_test, y_pred_knn, average='weighted'))\n",
    "rc_list.append(recall_score(y_test, y_pred_knn, average='weighted'))\n",
    "acc_list.append(accuracy_score(y_test, y_pred_knn))\n",
    "cm_list.append(confusion_matrix(y_test, y_pred_knn))\n",
    "\n",
    "# evaluate the CNN pipeline\n",
    "y_pred_cnn = cnn_pipeline.predict(X_test)\n",
    "f1_list.append(f1_score(y_test, y_pred_cnn, average='weighted'))\n",
    "pr_list.append(precision_score(y_test, y_pred_cnn, average='weighted'))\n",
    "rc_list.append(recall_score(y_test, y_pred_cnn, average='weighted'))\n",
    "acc_list.append(accuracy_score(y_test, y_pred_cnn))\n",
    "cm_list.append(confusion_matrix(y_test, y_pred_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['SVM', 'Kmeans', 'KNN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091e1e0",
   "metadata": {},
   "source": [
    "<a id='results'></a>\n",
    "<font size=\"+2\" color='#053c96'><b> 7. Results</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a65abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'Model': model_list, 'F1': f1_list, 'Precision': pr_list, 'Recall': rc_list, 'Accuracy': acc_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2626b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9d457",
   "metadata": {},
   "source": [
    "<a id='analysis-results'></a>\n",
    "\n",
    "<font size=\"+1\" color='#780404'><b> 7.1 Analysis of Results</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "The results suggest that the SVM and KNN models perform better than the Kmeans model, based on F1-score, precision, recall, and accuracy.  \n",
    "\n",
    "Overall, the SVM and KNN models appear to be more suitable for this task than the Kmeans model. However, the performance of the models may depend on the specific characteristics of the dataset, and further analysis may be necessary to confirm the generalizability of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea103b",
   "metadata": {},
   "source": [
    "<a id='model-performance'></a>\n",
    "\n",
    "<font size=\"+1\" color='#780404'><b> 7.2 Model Performance</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "From the given results, the SVM and KNN models perform better than the Kmeans model in terms of F1-score, precision, and recall. The SVM model has the highest F1-score and recall, while the KNN model has the highest precision. However, the SVM model has slightly lower precision than the KNN model.\n",
    "\n",
    "The Kmeans model has the lowest F1-score, precision, and recall among the three models, indicating that it performs poorly in predicting positive cases. Although the accuracy of the Kmeans model is the same as that of the SVM model, accuracy alone is not a reliable metric for imbalanced datasets where the majority class dominates the prediction.\n",
    "\n",
    "Therefore, based on the given results, the SVM and KNN models appear to be more suitable for the given task than the Kmeans model. However, it is important to note that the performance of the models may depend on the specific characteristics of the dataset, and further analysis may be necessary to confirm the generalizability of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b54eb5",
   "metadata": {},
   "source": [
    "<a id='implications'></a>\n",
    "\n",
    "<font size=\"+1\" color='#780404'><b> 7.3 Implications</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "The given results have several implications, such as:\n",
    "\n",
    "Model Selection: The results suggest that the SVM and KNN models may be more suitable for this task than the Kmeans model. Therefore, the SVM or KNN model may be selected for further analysis or implementation.\n",
    "\n",
    "Hyperparameter Tuning: The performance of the models may depend on the hyperparameters chosen. Therefore, hyperparameter tuning can be performed to optimize the model performance.\n",
    "\n",
    "Dataset Characteristics: The performance of the models may depend on the specific characteristics of the dataset, such as the distribution of the classes or the presence of outliers. Therefore, further analysis may be necessary to confirm the generalizability of the models to other datasets.\n",
    "\n",
    "Performance Metrics: The selection of performance metrics can have implications for the evaluation of the models. For example, the accuracy metric may not be reliable for imbalanced datasets. Therefore, multiple metrics should be used to evaluate the models comprehensively.\n",
    "\n",
    "Overall, the given results provide insights into the performance of the models and can guide further analysis or implementation of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb8f75",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "\n",
    "<font size=\"+2\" color='#053c96'><b> 8. Conclusion</b></font>  \n",
    "[back to top](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6ce88",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "\n",
    "<font size=\"+1\" color='#780404'><b> 8.1 Summary</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "In conclusion, the results of the machine learning models developed for music genre classification suggest that the SVM and KNN models perform better than the Kmeans model. Both the SVM and KNN models have higher F1-scores, precision, and recall than the Kmeans model, indicating better performance in predicting the different music genres, including Classically Punk. This implies that the developed machine learning application can automate the process of music genre classification, making it faster, more accurate, and scalable. The application can extract features from audio files and train an SVM or KNN model to classify music into different genres without the need for human intervention. However, further analysis is necessary to confirm the generalizability of the models to other datasets, and hyperparameter tuning may be required to optimize the performance of the models. Overall, the developed machine learning application has the potential to revolutionize the music industry by streamlining the process of music genre classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b300b",
   "metadata": {},
   "source": [
    "<a id='recommendations'></a>\n",
    "\n",
    "<font size=\"+1\" color='#780404'><b> 8.2 Recommendations</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "Based on the problem statement and the results obtained from the machine learning models, here are some recommendations that could help to further improve the music genre classification application:\n",
    "\n",
    "1. Data Augmentation: To improve the performance of the machine learning models, it may be helpful to augment the training data by adding variations of the existing data. This can be done by applying audio effects such as pitch shifting, time stretching, or adding noise to create a more diverse training set.\n",
    "\n",
    "2. Feature Engineering: Feature engineering involves selecting and extracting meaningful features from audio files to train the machine learning model. By experimenting with different audio features and selecting the most informative ones, the performance of the machine learning model can be improved.\n",
    "\n",
    "3. Ensemble Learning: Ensemble learning involves combining the predictions of multiple machine learning models to improve the overall performance. By training multiple models using different algorithms or hyperparameters and combining their predictions, the overall accuracy of the application can be improved.\n",
    "\n",
    "4. User Feedback: Collecting feedback from users of the application can help to improve the accuracy of the machine learning model over time. By allowing users to provide feedback on the accuracy of the classification results, the application can learn from its mistakes and continuously improve its performance.\n",
    "\n",
    "5. Continuous Monitoring: It is important to continuously monitor the performance of the machine learning model to ensure that it is accurately classifying the different music genres. Regularly updating the model with new data and retraining it can help to improve its accuracy and keep up with changes in the music industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3342fdf5",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "\n",
    "<font size=\"+2\" color='#053c96'><b> 9. References</b></font>  \n",
    "[back to top](#table-of-contents)  \n",
    "\n",
    "1. Arsh Chowdhry. Music Genre Classification Using CNN. https://blog.clairvoyantsoft.com/music-genre-classification-using-cnn-ef9461553726\n",
    "2. Faisal Ahmed, Padma Polash Paul, Marina Gavrilova. Music Genre Classification Using a Gradient-Based Local Texture Descriptor https://www.researchgate.net/publication/303860385_Music_Genre_Classification_Using_a_Gradient-Based_Local_Texture_Descriptor\n",
    "3. Albert Jimnez. Music Genre Classification with Deep Learning https://github.com/jsalbert/Music-Genre-Classification-with-Deep-Learning\n",
    "4. Insiyah Hajoori. Music Genre Classification https://github.com/Insiyaa/Music-Genre-Classification\n",
    "5. A. Elbir, N. Aydin. Music genre classification and music recommendation by using deep learning https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/el.2019.4202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af53da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
